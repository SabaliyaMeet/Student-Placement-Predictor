# -*- coding: utf-8 -*-
"""Pllacement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tfa45oOiVb5OtSPC3Oi0NPz7Q2uyvO8I
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('data/realistic_student_placement_data.csv')

df.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df.drop('placement', axis=1),
                                                    df['placement'],
                                                    test_size=0.2)

X_train.shape, X_test.shape

plt.scatter(df['cgpa'],df['iq'], c=df['placement'])

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler.mean_

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

np.round(X_train.describe(), 1)

np.round(X_train_scaled.describe(), 1)

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))

ax1.scatter(X_train['cgpa'], X_train['iq'])
ax1.set_title('before scaling Raw Data')
ax2.scatter(X_train_scaled['cgpa'], X_train_scaled['iq'], color='red')
ax2.set_title('after scaling Scaled Data')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))

ax1.set_title('before scaling Raw Data')
sns.kdeplot(X_train['cgpa'], ax=ax1)
sns.kdeplot(X_train['iq'], ax=ax1)

ax2.set_title('after scaling Scaled Data')
sns.kdeplot(X_train_scaled['cgpa'], ax=ax2)
sns.kdeplot(X_train_scaled['iq'], ax=ax2)

plt.show()

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf_scaled = LogisticRegression()

clf.fit(X_train,y_train)
clf_scaled.fit(X_train_scaled,y_train)

clf.predict(X_test)

X_test

y_pred = clf.predict(X_test)
y_pred_sacled = clf_scaled.predict(X_test_scaled)

from sklearn.metrics import accuracy_score

print("Actual", accuracy_score(y_test,y_pred))
print("Scaled", accuracy_score(y_test,y_pred_sacled))

from mlxtend.plotting import plot_decision_regions

plot_decision_regions(X_train_scaled.values, y_train.values, clf=clf_scaled)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Get predicted probabilities (only for class 1)
y_proba = clf_scaled.predict_proba(X_test_scaled)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Logistic (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

# Find the best threshold using Youden's J statistic
youden_index = tpr - fpr
best_threshold = thresholds[np.argmax(youden_index)]

print(f"✅ Best Threshold (Youden's J): {best_threshold:.2f}")

from sklearn.metrics import confusion_matrix, classification_report

# Classify based on the best threshold
y_pred_optimal = (y_proba >= best_threshold).astype(int)

# Evaluate performance
print("🔍 Confusion Matrix at Optimal Threshold:")
print(confusion_matrix(y_test, y_pred_optimal))

print("\n📊 Classification Report at Optimal Threshold:")
print(classification_report(y_test, y_pred_optimal))

# Compare with original threshold = 0.5
print("\n🔁 Classification Report at Default Threshold (0.5):")
print(classification_report(y_test, (y_proba >= 0.5).astype(int)))

# Create a DataFrame to show results
results_df = X_test.copy()  # Start with the test features
results_df['Actual Placement'] = y_test.values
results_df['Predicted Placement'] = y_pred_optimal  # From optimal threshold

# Optional: Reset index if needed
results_df.reset_index(drop=True, inplace=True)

# Display top 10 records
results_df.head(10)

# Create base DataFrame
results_df = X_test.copy()
results_df['Actual Placement'] = y_test.values
results_df['Predicted Probability'] = y_proba

# Predictions using default threshold (0.5)
results_df['Predicted@0.5'] = (y_proba >= 0.5).astype(int)

# Predictions using best threshold
results_df['Predicted@Best'] = y_pred_optimal

# Whether predictions match actual
results_df['Correct@0.5'] = results_df['Actual Placement'] == results_df['Predicted@0.5']
results_df['Correct@Best'] = results_df['Actual Placement'] == results_df['Predicted@Best']

# Mark which threshold each row used (for analysis or explanation)
results_df['Threshold Used'] = np.where(results_df['Predicted@0.5'] == results_df['Predicted@Best'],
                                        f"Same ({best_threshold:.2f})",
                                        f"Diff (0.5 vs {best_threshold:.2f})")

# Optional: Cleanup or formatting
results_df = results_df.rename(columns={
    'Predicted@0.5': 'Predicted (0.5)',
    'Predicted@Best': f'Predicted ({best_threshold:.2f})',
    'Correct@0.5': 'Correct (0.5)',
    'Correct@Best': f'Correct ({best_threshold:.2f})'
})

# Show top 10 mismatches between 0.5 and best
mismatches = results_df[results_df['Predicted (0.5)'] != results_df[f'Predicted ({best_threshold:.2f})']]
print("🔁 Mismatches between default and best threshold:", len(mismatches))
display(mismatches.head(10))




# Display top 10 full results
pd.set_option('display.max_rows', None)  # Shows all rows
display(results_df)

from sklearn.metrics import confusion_matrix

def get_confusion_details(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return {
        'TP': tp,
        'FP': fp,
        'FN': fn,
        'TN': tn
    }

# Get predictions
y_pred_05 = results_df['Predicted (0.5)']
y_pred_best = results_df[f'Predicted ({best_threshold:.2f})']

# Actual values
y_true = results_df['Actual Placement']

# Compute confusion matrix details
cm_05 = get_confusion_details(y_true, y_pred_05)
cm_best = get_confusion_details(y_true, y_pred_best)

# Display results
print("📊 Confusion Matrix Metrics:")

print("\n🔘 Threshold = 0.5")
for k, v in cm_05.items():
    print(f"{k}: {v}")

print(f"\n🟢 Threshold = {best_threshold:.2f}")
for k, v in cm_best.items():
    print(f"{k}: {v}")

